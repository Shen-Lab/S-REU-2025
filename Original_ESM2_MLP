import os
import numpy as np
import torch
from torch.utils.data import DataLoader, TensorDataset, random_split
from torch import nn, optim
from scipy.stats import spearmanr

# Device setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Data directory
data_dir = "/scratch/user/gaygysyz2003/supervised/MEP-SiPLM-main/supervised/embeddingstest"
embedding_files = sorted(f for f in os.listdir(data_dir) if f.endswith("_embeddings.npy"))

# Collect all data
all_embeddings, all_labels = [], []

for emb_file in embedding_files:
    assay_name = emb_file.replace("_embeddings.npy", "")
    emb_path = os.path.join(data_dir, emb_file)
    label_path = os.path.join(data_dir, f"{assay_name}_labels.npy")
    
    if not os.path.exists(label_path):
        print(f"‚ö†Ô∏è Skipping {assay_name}, no label file found.")
        continue

    embeddings = np.load(emb_path)
    labels = np.load(label_path).squeeze()

    if len(embeddings) != len(labels):
        print(f"‚ö†Ô∏è Skipping {assay_name}, mismatched lengths.")
        continue
    if len(labels) < 10:
        print(f"‚ö†Ô∏è Skipping {assay_name}, too few samples.")
        continue
    
    std = labels.std()
    if std == 0:
        print(f"‚ö†Ô∏è Skipping {assay_name}, label std = 0 (constant labels)")
        continue
    
    labels = (labels - labels.mean()) / std
    all_embeddings.append(embeddings)
    all_labels.append(labels)

if not all_embeddings:
    raise ValueError("‚ùå No valid data found!")

# Combine
X = torch.tensor(np.concatenate(all_embeddings, axis=0), dtype=torch.float32)
y = torch.tensor(np.concatenate(all_labels, axis=0), dtype=torch.float32)

# Train/val/test split
dataset = TensorDataset(X, y)
total = len(dataset)
train_sz = int(total * 0.8)
val_sz = int(total * 0.1)
test_sz = total - train_sz - val_sz

train_set, val_set, test_set = random_split(
    dataset,
    [train_sz, val_sz, test_sz],
    generator=torch.Generator().manual_seed(42)
)
train_loader = DataLoader(train_set, batch_size=32, shuffle=True)
val_loader = DataLoader(val_set, batch_size=32)
test_loader = DataLoader(test_set, batch_size=32)

# Model
class SimpleMLP(nn.Module):
    def __init__(self, input_dim, hidden_dim=256, dropout_p=0.1):
        super().__init__()
        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Dropout(dropout_p)]
        for _ in range(8):
            layers += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Dropout(dropout_p)]
        layers.append(nn.Linear(hidden_dim, 1))
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)

model = SimpleMLP(input_dim=X.shape[1], hidden_dim=256, dropout_p=0.1).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)
loss_fn = nn.MSELoss()

best_val_loss = float("inf")
for epoch in range(1, 101):
    model.train()
    train_loss = 0.
    for xb, yb in train_loader:
        xb, yb = xb.to(device), yb.to(device)
        optimizer.zero_grad()
        loss = loss_fn(model(xb).reshape(-1), yb)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    model.eval()
    val_loss = 0.
    with torch.no_grad():
        for xb, yb in val_loader:
            xb, yb = xb.to(device), yb.to(device)
            val_loss += loss_fn(model(xb).reshape(-1), yb).item()

    train_loss /= len(train_loader)
    val_loss /= len(val_loader)
    print(f"Epoch {epoch}: Train Loss = {train_loss:.4f} | Val Loss = {val_loss:.4f}")
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_state = model.state_dict()

model.load_state_dict(best_state)
torch.save(model.state_dict(), "trained_global_model.pt")
print("üíæ Model saved as 'trained_global_model.pt'")

# Test evaluation
preds, targets = [], []
model.eval()
with torch.no_grad():
    for xb, yb in test_loader:
        xb, yb = xb.to(device), yb.to(device)
        preds.extend(model(xb).reshape(-1).cpu().numpy())
        targets.extend(yb.cpu().numpy())

if len(set(targets)) > 1:
    corr = spearmanr(preds, targets).correlation
    print(f"‚úÖ Spearman Correlation on Test Set: {corr:.4f}")
    for i in range(min(3, len(preds))):
        print(f"  Prediction: {preds[i]:.4f} | True Label: {targets[i]:.4f}")
else:
    print("‚ö†Ô∏è Skipped Spearman: constant test labels.")
